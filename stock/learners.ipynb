{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNntIrs9/vbdyGMjb1qnM72"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**코드 조각 1: 학습기 모듈의 의존성 임포트**"],"metadata":{"id":"pGacRkAMd0TC"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vW1NFX3EljC4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699695284834,"user_tz":-540,"elapsed":38283,"user":{"displayName":"빈투스마일루스","userId":"05197320515904642127"}},"outputId":"96cde641-fe5f-4e18-86a9-dece80cccb62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/stock\n","!pip install import_ipynb\n","import import_ipynb"],"metadata":{"id":"maQURZoXjyLV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699695314506,"user_tz":-540,"elapsed":9428,"user":{"displayName":"빈투스마일루스","userId":"05197320515904642127"}},"outputId":"e2477576-38de-4fc7-dd4f-a0d5340cecc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/stock\n","Collecting import_ipynb\n","  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (7.34.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (5.9.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (67.7.2)\n","Collecting jedi>=0.16 (from IPython->import_ipynb)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.8.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (2.18.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (4.19.2)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (5.5.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.12.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.9)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->import_ipynb) (3.11.0)\n","Installing collected packages: jedi, import_ipynb\n","Successfully installed import_ipynb-0.1.4 jedi-0.19.1\n"]}]},{"cell_type":"code","source":["%run /content/drive/MyDrive/stock/utils.ipynb\n","%run /content/drive/MyDrive/stock/Environment.ipynb\n","%run /content/drive/MyDrive/stock/agent.ipynb\n","%run /content/drive/MyDrive/stock/networks.ipynb\n","%run /content/drive/MyDrive/stock/visualizer.ipynb\n","%run /content/drive/MyDrive/stock/settings.ipynb"],"metadata":{"id":"G60gmt9C-bn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import logging\n","import abc\n","import collections\n","import threading\n","import time\n","import json\n","import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"v-5NiWahb7-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger(LOGGER_NAME)\n","\n","class ReinforcementLearner:\n","    __metaclass__ = abc.ABCMeta\n","    lock = threading.Lock()\n","\n","    def __init__(self, method='a2c', stock_code=None,\n","                chart_data=None, training_data=None,\n","                min_trading_price=100000, max_trading_price=10000000,\n","                net='lstm', num_steps=1, lr=0.0005,\n","                discount_factor=0.9, num_epoches=100,\n","                balance=100000000, start_epsilon=1,\n","                value_network=None, policy_network=None,\n","                output_path='', reuse_models=True, gen_output=True):\n","        # 인자 확인\n","        assert min_trading_price > 0\n","        assert max_trading_price > 0\n","        assert max_trading_price >= min_trading_price\n","        assert num_steps > 0\n","        assert lr > 0\n","        # 강화학습 설정\n","        self.method = method\n","        self.discount_factor = discount_factor\n","        self.num_epoches = num_epoches\n","        self.start_epsilon = start_epsilon\n","        # 환경 설정\n","        self.stock_code = stock_code\n","        self.chart_data = chart_data\n","        self.environment = Environment(chart_data)\n","        # 에이전트 설정\n","        self.agent = Agent(self.environment, balance, min_trading_price, max_trading_price)\n","        # 학습 데이터\n","        self.training_data = training_data\n","        self.sample = None\n","        self.training_data_idx = -1\n","        # 벡터 크기 = 학습 데이터 벡터 크기 + 에이전트 상태 크기\n","        self.num_features = self.agent.STATE_DIM\n","        if self.training_data is not None:\n","            self.num_features += self.training_data.shape[1]\n","        # 신경망 설정\n","        self.net = net\n","        self.num_steps = num_steps\n","        self.lr = lr\n","        self.value_network = value_network\n","        self.policy_network = policy_network\n","        self.reuse_models = reuse_models\n","        # 가시화 모듈\n","        self.visualizer = Visualizer()\n","        # 메모리\n","        self.memory_sample = []\n","        self.memory_action = []\n","        self.memory_reward = []\n","        self.memory_value = []\n","        self.memory_policy = []\n","        self.memory_pv = []\n","        self.memory_num_stocks = []\n","        self.memory_exp_idx = []\n","        # 에포크 관련 정보\n","        self.loss = 0.\n","        self.itr_cnt = 0\n","        self.exploration_cnt = 0\n","        self.batch_size = 0\n","        # 로그 등 출력 경로\n","        self.output_path = output_path\n","        self.gen_output = gen_output\n","\n","    def init_value_network(self, shared_network=None, activation='linear', loss='mse'):\n","        if self.net == 'dnn':\n","            self.value_network = DNN(\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS,\n","                lr=self.lr, shared_network=shared_network,\n","                activation=activation, loss=loss)\n","        elif self.net == 'lstm':\n","            self.value_network = LSTMNetwork(\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS,\n","                lr=self.lr, num_steps=self.num_steps,\n","                shared_network=shared_network,\n","                activation=activation, loss=loss)\n","        elif self.net == 'cnn':\n","            self.value_network = CNN(\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS,\n","                lr=self.lr, num_steps=self.num_steps,\n","                shared_network=shared_network,\n","                activation=activation, loss=loss)\n","        if self.reuse_models and os.path.exists(self.value_network_path):\n","            self.value_network.load_model(model_path=self.value_network_path)\n","\n","    def init_policy_network(self, shared_network=None, activation='sigmoid',\n","                            loss='binary_crossentropy'):\n","        if self.net == 'dnn':\n","            self.policy_network = DNN(\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS,\n","                lr=self.lr, shared_network=shared_network,\n","                activation=activation, loss=loss)\n","        elif self.net == 'lstm':\n","            self.policy_network = LSTMNetwork(\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS,\n","                lr=self.lr, num_steps=self.num_steps,\n","                shared_network=shared_network,\n","                activation=activation, loss=loss)\n","        elif self.net == 'cnn':\n","            self.policy_network = CNN(\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS,\n","                lr=self.lr, num_steps=self.num_steps,\n","                shared_network=shared_network,\n","                activation=activation, loss=loss)\n","        if self.reuse_models and os.path.exists(self.policy_network_path):\n","            self.policy_network.load_model(model_path=self.policy_network_path)\n","\n","    def reset(self):\n","        self.sample = None\n","        self.training_data_idx = -1\n","        # 환경 초기화\n","        self.environment.reset()\n","        # 에이전트 초기화\n","        self.agent.reset()\n","        # 가시화 초기화\n","        self.visualizer.clear([0, len(self.chart_data)])\n","        # 메모리 초기화\n","        self.memory_sample = []\n","        self.memory_action = []\n","        self.memory_reward = []\n","        self.memory_value = []\n","        self.memory_policy = []\n","        self.memory_pv = []\n","        self.memory_num_stocks = []\n","        self.memory_exp_idx = []\n","        # 에포크 관련 정보 초기화\n","        self.loss = 0.\n","        self.itr_cnt = 0\n","        self.exploration_cnt = 0\n","        self.batch_size = 0\n","\n","    def build_sample(self):\n","        self.environment.observe()\n","        if len(self.training_data) > self.training_data_idx + 1:\n","            self.training_data_idx += 1\n","            self.sample = self.training_data[self.training_data_idx, :].tolist()\n","            self.sample.extend(self.agent.get_states())\n","            return self.sample\n","        return None\n","\n","    @abc.abstractmethod\n","    def get_batch(self):\n","        pass\n","\n","    def fit(self):\n","        # 배치 학습 데이터 생성\n","        x, y_value, y_policy = self.get_batch()\n","        # 손실 초기화\n","        self.loss = None\n","        if len(x) > 0:\n","            loss = 0\n","            if y_value is not None:\n","                # 가치 신경망 갱신\n","                loss += self.value_network.train_on_batch(x, y_value)\n","            if y_policy is not None:\n","                # 정책 신경망 갱신\n","                loss += self.policy_network.train_on_batch(x, y_policy)\n","            self.loss = loss\n","\n","    def visualize(self, epoch_str, num_epoches, epsilon):\n","        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action\n","        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks\n","        if self.value_network is not None:\n","            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] \\\n","                                * (self.num_steps - 1) + self.memory_value\n","        if self.policy_network is not None:\n","            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] \\\n","                                * (self.num_steps - 1) + self.memory_policy\n","        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv\n","        self.visualizer.plot(\n","            epoch_str=epoch_str, num_epoches=num_epoches,\n","            epsilon=epsilon, action_list=Agent.ACTIONS,\n","            actions=self.memory_action,\n","            num_stocks=self.memory_num_stocks,\n","            outvals_value=self.memory_value,\n","            outvals_policy=self.memory_policy,\n","            exps=self.memory_exp_idx,\n","            initial_balance=self.agent.initial_balance,\n","            pvs=self.memory_pv,\n","        )\n","        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))\n","\n","    def run(self, learning=True):\n","        info = (\n","            f'[{self.stock_code}] RL:{self.method} NET:{self.net} '\n","            f'LR:{self.lr} DF:{self.discount_factor} '\n","        )\n","        with self.lock:\n","            logger.debug(info)\n","\n","        # 시작 시간\n","        time_start = time.time()\n","\n","        # 가시화 준비\n","        # 차트 데이터는 변하지 않으므로 미리 가시화\n","        self.visualizer.prepare(self.chart_data, info)\n","\n","        # 가시화 결과 저장할 폴더 준비\n","        if self.gen_output:\n","            self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}_{get_time_str()}')\n","            if not os.path.isdir(self.epoch_summary_dir):\n","                os.makedirs(self.epoch_summary_dir)\n","            else:\n","                for f in os.listdir(self.epoch_summary_dir):\n","                    os.remove(os.path.join(self.epoch_summary_dir, f))\n","\n","        # 학습에 대한 정보 초기화\n","        max_portfolio_value = 0\n","        epoch_win_cnt = 0\n","\n","        # 에포크 반복\n","        for epoch in tqdm(range(self.num_epoches)):\n","            time_start_epoch = time.time()\n","\n","            # step 샘플을 만들기 위한 큐\n","            q_sample = collections.deque(maxlen=self.num_steps)\n","\n","            # 환경, 에이전트, 신경망, 가시화, 메모리 초기화\n","            self.reset()\n","\n","            # 학습을 진행할 수록 탐험 비율 감소\n","            if learning:\n","                epsilon = self.start_epsilon * (1 - (epoch / (self.num_epoches - 1)))\n","            else:\n","                epsilon = self.start_epsilon\n","\n","            for i in tqdm(range(len(self.training_data)), leave=False):\n","                # 샘플 생성\n","                next_sample = self.build_sample()\n","                if next_sample is None:\n","                    break\n","\n","                # num_steps만큼 샘플 저장\n","                q_sample.append(next_sample)\n","                if len(q_sample) < self.num_steps:\n","                    continue\n","\n","                # 가치, 정책 신경망 예측\n","                pred_value = None\n","                pred_policy = None\n","                if self.value_network is not None:\n","                    pred_value = self.value_network.predict(list(q_sample))\n","                if self.policy_network is not None:\n","                    pred_policy = self.policy_network.predict(list(q_sample))\n","\n","                # 신경망 또는 탐험에 의한 행동 결정\n","                action, confidence, exploration = \\\n","                    self.agent.decide_action(pred_value, pred_policy, epsilon)\n","\n","                # 결정한 행동을 수행하고 보상 획득\n","                reward = self.agent.act(action, confidence)\n","\n","                # 행동 및 행동에 대한 결과를 기억\n","                self.memory_sample.append(list(q_sample))\n","                self.memory_action.append(action)\n","                self.memory_reward.append(reward)\n","                if self.value_network is not None:\n","                    self.memory_value.append(pred_value)\n","                if self.policy_network is not None:\n","                    self.memory_policy.append(pred_policy)\n","                self.memory_pv.append(self.agent.portfolio_value)\n","                self.memory_num_stocks.append(self.agent.num_stocks)\n","                if exploration:\n","                    self.memory_exp_idx.append(self.training_data_idx)\n","\n","                # 반복에 대한 정보 갱신\n","                self.batch_size += 1\n","                self.itr_cnt += 1\n","                self.exploration_cnt += 1 if exploration else 0\n","\n","            # 에포크 종료 후 학습\n","            if learning:\n","                self.fit()\n","\n","            # 에포크 관련 정보 로그 기록\n","            num_epoches_digit = len(str(self.num_epoches))\n","            epoch_str = str(epoch + 1).rjust(num_epoches_digit, '0')\n","            time_end_epoch = time.time()\n","            elapsed_time_epoch = time_end_epoch - time_start_epoch\n","            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}/{self.num_epoches}] '\n","                f'Epsilon:{epsilon:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '\n","                f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '\n","                f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '\n","                f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')\n","\n","            # 에포크 관련 정보 가시화\n","            if self.gen_output:\n","                if self.num_epoches == 1 or (epoch + 1) % max(int(self.num_epoches / 10), 1) == 0:\n","                    self.visualize(epoch_str, self.num_epoches, epsilon)\n","\n","            # 학습 관련 정보 갱신\n","            max_portfolio_value = max(\n","                max_portfolio_value, self.agent.portfolio_value)\n","            if self.agent.portfolio_value > self.agent.initial_balance:\n","                epoch_win_cnt += 1\n","\n","        # 종료 시간\n","        time_end = time.time()\n","        elapsed_time = time_end - time_start\n","\n","        # 학습 관련 정보 로그 기록\n","        with self.lock:\n","            logger.debug(f'[{self.stock_code}] Elapsed Time:{elapsed_time:.4f} '\n","                f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')\n","\n","    def save_models(self):\n","        if self.value_network is not None and self.value_network_path is not None:\n","            self.value_network.save_model(self.value_network_path)\n","        if self.policy_network is not None and self.policy_network_path is not None:\n","            self.policy_network.save_model(self.policy_network_path)\n","\n","    def predict(self):\n","        # 에이전트 초기화\n","        self.agent.reset()\n","\n","        # step 샘플을 만들기 위한 큐\n","        q_sample = collections.deque(maxlen=self.num_steps)\n","\n","        result = []\n","        while True:\n","            # 샘플 생성\n","            next_sample = self.build_sample()\n","            if next_sample is None:\n","                break\n","\n","            # num_steps만큼 샘플 저장\n","            q_sample.append(next_sample)\n","            if len(q_sample) < self.num_steps:\n","                continue\n","\n","            # 가치, 정책 신경망 예측\n","            pred_value = None\n","            pred_policy = None\n","            if self.value_network is not None:\n","                pred_value = self.value_network.predict(list(q_sample)).tolist()\n","            if self.policy_network is not None:\n","                pred_policy = self.policy_network.predict(list(q_sample)).tolist()\n","\n","            # 신경망에 의한 행동 결정\n","            result.append((self.environment.observation[0], pred_value, pred_policy))\n","\n","        if self.gen_output:\n","            with open(os.path.join(self.output_path, f'pred_{self.stock_code}.json'), 'w') as f:\n","                print(json.dumps(result), file=f)\n","        return result\n","\n","class ActorCriticLearner(ReinforcementLearner):\n","    def __init__(self, *args, shared_network=None,\n","        value_network_path=None, policy_network_path=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        if shared_network is None:\n","            self.shared_network = Network.get_shared_network(\n","                net=self.net, num_steps=self.num_steps,\n","                input_dim=self.num_features,\n","                output_dim=self.agent.NUM_ACTIONS)\n","        else:\n","            self.shared_network = shared_network\n","        self.value_network_path = value_network_path\n","        self.policy_network_path = policy_network_path\n","        if self.value_network is None:\n","            self.init_value_network(shared_network=self.shared_network)\n","        if self.policy_network is None:\n","            self.init_policy_network(shared_network=self.shared_network)\n","\n","    def get_batch(self):\n","        memory = zip(\n","            reversed(self.memory_sample),\n","            reversed(self.memory_action),\n","            reversed(self.memory_value),\n","            reversed(self.memory_policy),\n","            reversed(self.memory_reward),\n","        )\n","        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n","        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n","        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n","        value_max_next = 0\n","        for i, (sample, action, value, policy, reward) in enumerate(memory):\n","            x[i] = sample\n","            r = self.memory_reward[-1] - reward\n","            y_value[i, :] = value\n","            y_value[i, action] = r + self.discount_factor * value_max_next\n","            y_policy[i, :] = policy\n","            y_policy[i, action] = sigmoid(r)\n","            value_max_next = value.max()\n","        return x, y_value, y_policy\n","\n","\n","class A2CLearner(ActorCriticLearner):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","    def get_batch(self):\n","        memory = zip(\n","            reversed(self.memory_sample),\n","            reversed(self.memory_action),\n","            reversed(self.memory_value),\n","            reversed(self.memory_policy),\n","            reversed(self.memory_reward),\n","        )\n","        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n","        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n","        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n","        value_max_next = 0\n","        reward_next = self.memory_reward[-1]\n","        for i, (sample, action, value, policy, reward) in enumerate(memory):\n","            x[i] = sample\n","            r = reward_next + self.memory_reward[-1] - reward * 2\n","            reward_next = reward\n","            y_value[i, :] = value\n","            y_value[i, action] = np.tanh(r + self.discount_factor * value_max_next)\n","            advantage = y_value[i, action] - y_value[i].mean()\n","            y_policy[i, :] = policy\n","            y_policy[i, action] = sigmoid(advantage)\n","            value_max_next = value.max()\n","        return x, y_value, y_policy"],"metadata":{"id":"AMDnv98jnmq-"},"execution_count":null,"outputs":[]}]}